{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#open the json and store it\n",
    "with open(\"intents.json\") as intents:\n",
    "    intent_data = json.load(intents)\n",
    "# intent_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#will hold all words\n",
    "all_words = []\n",
    "\n",
    "#will hold all possible intent tags\n",
    "all_labels = []\n",
    "\n",
    "#will hold all pattern data, each pattern in a list of itself\n",
    "all_patterns = []\n",
    "\n",
    "#will hold the type of intent of the corresponding word in all_patterns\n",
    "all_responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop through the intent dictionary\n",
    "for intent in intent_data['intents']:\n",
    "    #loop through each pattern in each patterns list\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize basically separates each sentence into individual words\n",
    "        words = nltk.word_tokenize(pattern)\n",
    "        #add all the words into all words\n",
    "        all_words.extend(words)\n",
    "        all_patterns.append(words)\n",
    "        all_responses.append(intent['tag'])\n",
    "        \n",
    "    if intent['tag'] not in all_labels:\n",
    "        all_labels.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'How', 'are', 'you', 'hey']\n",
      "['greeting', 'goodbye', 'name', 'name_yes', 'name_no', 'thanks', 'first', 'lc_initial', 'lc_main', 'bc_initial', 'bc_main', 'sc_main']\n",
      "[['Hi'], ['How', 'are', 'you'], ['hey'], ['yo'], ['Is', 'anyone', 'there']]\n",
      "['greeting', 'greeting', 'greeting', 'greeting', 'greeting']\n"
     ]
    }
   ],
   "source": [
    "print(all_words[:5])\n",
    "print(all_labels)\n",
    "print(all_patterns[:5])\n",
    "print(all_responses[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'how', 'are', 'you', 'hey', 'yo', 'is', 'anyon', 'there', 'hello', 'good', 'day', 'what', 'up', 'sup', 'cya', 'see', 'you', 'later', 'goodby', 'im', 'leav', 'have', 'a', 'good', 'day', 'bye', 'what', 'is', 'your', 'name', 'what', 'should', 'i', 'call', 'you', 'what', 'your', 'name', 'name', 'your', 'name', 'yes', 'haha', 'yes', 'yep', 'nice', 'inde', 'yup', 'yea', 'nah', 'no', 'nope', 'not', 'realli', 'ew', 'cring', 'thank', 'thank', 'you', 'that', 'help', 'awesom', 'thank', 'thank', 'for', 'help', 'me', 'sick', 'i', 'dont', 'feel', 'good', 'not', 'well', 'i', 'am', 'not', 'feel', 'well', 'i', 'feel', 'sick', 'someth', 'is', 'wrong', 'with', 'me', 'i', 'do', 'not', 'think', 'i', 'am', 'of', 'perfect', 'health', 'cough', 'i', 'have', 'a', 'cough', 'i', 'have', 'a', 'chest', 'pain', 'chest', 'pain', 'i', 'have', 'a', 'chest', 'ach', 'chest', 'ach', 'i', 'feel', 'unusu', 'tire', 'i', 'feel', 'too', 'tire', 'i', 'feel', 'exhaust', 'i', 'get', 'tire', 'too', 'easili', 'no', 'energi', 'lack', 'in', 'energi', 'i', 'dont', 'want', 'to', 'eat', 'i', 'dont', 'have', 'an', 'appetit', 'i', 'dont', 'want', 'to', 'eat', 'cough', 'blood', 'i', 'cough', 'blood', 'out', 'of', 'breath', 'breath', 'difficulti', 'hard', 'to', 'breath', 'non', 'stop', 'cough', 'long', 'cough', 'breast', 'size', 'chang', 'my', 'breast', 'look', 'larger', 'than', 'usual', 'my', 'breast', 'ha', 'swell', 'my', 'breast', 'is', 'swollen', 'breast', 'swell', 'breast', 'darken', 'breast', 'warmth', 'breast', 'ach', 'my', 'breast', 'pain', 'shoulder', 'lump', 'breast', 'lump', 'i', 'have', 'a', 'lump', 'on', 'my', 'breast', 'i', 'have', 'a', 'lump', 'on', 'my', 'shoulder', 'nippl', 'skin', 'flake', 'skin', 'around', 'nippl', 'unusu', 'peel', 'of', 'skin', 'around', 'nippl', 'unheal', 'sore', 'sore', 'not', 'heal', 'waxi', 'bump', 'pear', 'bump', 'brown', 'scar', 'crusti', 'skin', 'return', 'scab', 'mole', 'brown', 'spot', 'pain', 'lesion', 'dark', 'lesion']\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "#removes the ends of the words. Basically reducing the words to their root type\n",
    "#for example if there was a word that is \"Whats\" it removes the \"s\" returning only \"what\"\n",
    "#the reason for doing this is to get the plain meaning of the word whilst ignoring any unnecessary\n",
    "#additions that might confuse the model, making it to be able to generalize more\n",
    "all_words = [lemmatizer.lemmatize(word.lower()) for word in all_words]\n",
    "all_words = [stemmer.stem(word.lower()) for word in all_words if word]\n",
    "print(all_words)\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove any duplicate words, and sort for easiness\n",
    "all_words = sorted(list(set(all_words)))\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "all_labels = sorted(all_labels)\n",
    "print(len(all_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A neural network cannot interpret these Strings\n",
    "#However, they can be one-hot encoded to numbers\n",
    "#One hot encoding - Bag of Words (if word is there - \"hot\", represented with a 1)\n",
    "#the mapping is [the, she, he, him, they, was, a, guy, person]\n",
    "#one-hot representation -> [0, 0, 1, 0, 0, 1, 1, 0, 1], for the sentence \"he was a person\"\n",
    "#We'll use this representation for each sentence, using {all_words} as its list to mapped against\n",
    "\n",
    "#training and output list\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "#creating a list of all 0's  to use as a starting point\n",
    "out_empty = [0 for _ in range(len(all_labels))]\n",
    "# out_empty -> [0,0,0,0,0,0]\n",
    "\n",
    "for index, pattern in enumerate(all_patterns):\n",
    "    #the bag of words\n",
    "    bag = []\n",
    "    words = [lemmatizer.lemmatize(word.lower()) for word in pattern]\n",
    "    words = [stemmer.stem(word.lower()) for word in words]\n",
    "    \n",
    "    for word in all_words:\n",
    "        #loop and check whether each word consists\n",
    "        if word in words:\n",
    "            #if it does append 1\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "    \n",
    "    #create a copy of out_empty\n",
    "    output_row = out_empty[:]\n",
    "    \n",
    "    #set the position of the tag equal to 1\n",
    "    output_row[all_labels.index(all_responses[index])] = 1\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "    \n",
    "#convert to arrays, for tflearn to accept\n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)\n",
    "#the data is now ready to be used to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\initializations.py:165: calling TruncatedNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "(94, 124)\n",
      "(94, 12)\n"
     ]
    }
   ],
   "source": [
    "#input data size - bag of words length\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "#hidden layers\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "#output layer, size equal to number of possibilities\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "#Regular deep neural network\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "print(training.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Run id: HOHF0W\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "---------------------------------\n",
      "Training samples: 94\n",
      "Validation samples: 0\n",
      "--\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-ca1c7e03196e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#     model.load(\"chatbot.tflearn\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# except:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_inputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_targets\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_metric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"chatbot.tflearn\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[0;32m    204\u001b[0m                          \u001b[0mexcl_trainops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexcl_trainops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                          \u001b[0mrun_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                          callbacks=callbacks)\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mretrieve_data_preprocessing_and_augmentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, feed_dicts, n_epoch, val_feed_dicts, show_metric, snapshot_step, snapshot_epoch, shuffle_all, dprep_dict, daug_dict, excl_trainops, run_id, callbacks)\u001b[0m\n\u001b[0;32m    342\u001b[0m                                                        \u001b[1;33m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_checkpoint_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m|\u001b[0m \u001b[0msnapshot_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m                                                        \u001b[0msnapshot_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m                                                        show_metric)\n\u001b[0m\u001b[0;32m    345\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m                             \u001b[1;31m# Update training state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\helpers\\trainer.py\u001b[0m in \u001b[0;36m_train\u001b[1;34m(self, training_step, snapshot_epoch, snapshot_step, show_metric)\u001b[0m\n\u001b[0;32m    824\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m         \u001b[0mfeed_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 826\u001b[1;33m         \u001b[0mtflearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    827\u001b[0m         _, train_summ_str = self.session.run([self.train, self.summ_op],\n\u001b[0;32m    828\u001b[0m                                              feed_batch)\n",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tflearn\\config.py\u001b[0m in \u001b[0;36mis_training\u001b[1;34m(is_training, session)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0minit_training_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'is_training_ops'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'is_training_ops'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36meval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[0mA\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0marray\u001b[0m \u001b[0mcorresponding\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m     \"\"\"\n\u001b[1;32m--> 913\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_eval_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    914\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mdeprecation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Use ref() instead.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5510\u001b[0m                        \u001b[1;34m\"the tensor's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5511\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 5512\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 958\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    959\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\ammuuu\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "# try:\n",
    "#     model.load(\"chatbot.tflearn\")\n",
    "# except:\n",
    "model.fit(X_inputs=training, Y_targets=output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"chatbot.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the input text into bag of words\n",
    "def bag_of_words(text, all_words):\n",
    "    bag = [0 for _ in range(len(all_words))]\n",
    "                            \n",
    "    text_words = nltk.word_tokenize(text)\n",
    "    text_words = [lemmatizer.lemmatize(word.lower()) for word in text_words]\n",
    "    text_words = [stemmer.stem(word.lower()) for word in text_words]\n",
    "    \n",
    "    for wrd in text_words:\n",
    "        for index, word in enumerate(all_words):\n",
    "            if word == wrd:\n",
    "                bag[index] = 1\n",
    "    \n",
    "    return numpy.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(username):\n",
    "    print(f'Hi {username}, how can I help you today?')\n",
    "    \n",
    "    #Reset context on start, due to there being no context\n",
    "    context = None\n",
    "    #default responses if no proper valid match\n",
    "    default_responses = [\n",
    "    \"Sorry, can't understand you, I am not perfect :'(\", \"Please give me more info :(\", \"Not sure I understand :(\",\n",
    "    \"Please be more specific\", \"Please provide me more information\"\n",
    "    ]\n",
    "    \n",
    "    while True:\n",
    "        user_input = str(input(\"You: \")).lower()\n",
    "        if user_input == 'quit':\n",
    "            break\n",
    "    \n",
    "        #will hold a list of probabilities (softmax)\n",
    "        bag = bag_of_words(user_input, all_words)\n",
    "        \n",
    "        results = model.predict([bag])[0]\n",
    "        #will return index of highest probability\n",
    "        result_index = numpy.argmax(results)\n",
    "        #Corresponding tag of prediction\n",
    "        result_tag = all_labels[result_index]\n",
    "           \n",
    "#         print(\"Prediction Probability\", results[result_index])\n",
    "            \n",
    "        #only if the model is quite confident do this\n",
    "        if results[result_index] > 0.8:\n",
    "            #break loop if end\n",
    "            if result_tag == 'goodbye' or result_tag == 'thanks':\n",
    "                responses = intent_data['intents'][1]['responses'] if result_tag == 'goodbye' else intent_data['intents'][5]['responses']\n",
    "                print(\"CHANCO: \" + random.choice(responses))\n",
    "                break\n",
    "            \n",
    "            for intent in intent_data['intents']:\n",
    "                #predicted intent\n",
    "                if intent['tag'] == result_tag:\n",
    "                    print(result_tag)\n",
    "#                     print('context_filter' in intent and intent['context_filter'] == context)\n",
    "\n",
    "                    #this if condition checks to see whether the context_filter intent has the same value as the context of the\n",
    "                    #intent it is referring to, if so it means that the context_filter intent will be given authority to provide a response\n",
    "                    if 'context_filter' not in intent or 'context_filter' in intent and intent['context_filter'] == context:\n",
    "                        #responses of corresponding intent\n",
    "                        responses = intent['responses']\n",
    "                        \n",
    "                        #does the current intent have context\n",
    "                        if 'context' in intent:\n",
    "                            context = intent['context']\n",
    "                        else:\n",
    "                            context = None\n",
    "\n",
    "                        #choose some random response\n",
    "                        print(\"CHANCO: \" + random.choice(responses))\n",
    "\n",
    "                    #if user enters smth directly about the main cancer symptoms\n",
    "                    elif intent.get('direct_access'):\n",
    "                        responses = intent['responses']\n",
    "                        print(\"CHANCO: \" + random.choice(responses))\n",
    "                        \n",
    "                    else:\n",
    "                        print(\"CHANCO: \" + random.choice(default_responses))\n",
    "        \n",
    "        #if not so confident - print a default text\n",
    "        else :\n",
    "            print(\"CHANCO: \" + random.choice(default_responses))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi bro, how can I help you today?\n",
      "You: dd\n",
      "Prediction Probability 0.922012\n",
      "name_no\n",
      "False\n",
      "CHANCO: Please give me more info :(\n",
      "\n",
      "You: hey\n",
      "Prediction Probability 0.99918944\n",
      "greeting\n",
      "False\n",
      "CHANCO: Good to see you again!\n",
      "\n",
      "You: how are you\n",
      "Prediction Probability 0.9999857\n",
      "greeting\n",
      "False\n",
      "CHANCO: Good to see you again!\n",
      "\n",
      "You: ur name\n",
      "Prediction Probability 0.9983878\n",
      "name\n",
      "False\n",
      "CHANCO: I am CHANCO! does it fit?\n",
      "\n",
      "You: yep\n",
      "Prediction Probability 0.9972209\n",
      "name_yes\n",
      "True\n",
      "CHANCO: Yayyy thankyou!\n",
      "\n",
      "You: sick\n",
      "Prediction Probability 0.9950734\n",
      "first\n",
      "False\n",
      "CHANCO: oh is that so :( Not to worry I am the 'chanciest' :P out there, please tell me, what do you have?\n",
      "\n",
      "You: cough\n",
      "Prediction Probability 0.96734947\n",
      "lc_initial\n",
      "False\n",
      "CHANCO: I am sorry to hear that... These symptoms however can be less serious than you think! :) So please let me know further symptoms that you might have (Coughing up blood, a cough that does not stop, breathlessness)\n",
      "\n",
      "You: yes\n",
      "Prediction Probability 0.9989372\n",
      "name_yes\n",
      "False\n",
      "CHANCO: Please give me more info :(\n",
      "\n",
      "You: cough blood\n",
      "Prediction Probability 0.9978649\n",
      "lc_main\n",
      "True\n",
      "CHANCO: I suspect a chance of you having lung cancer... But there's always a chance of positivity! I suggest that you please get an x-ray uploaded on our detection section so that we can confirm\n",
      "\n",
      "You: thanks\n",
      "Prediction Probability 0.9778706\n",
      "CHANCO: Happy to help!\n"
     ]
    }
   ],
   "source": [
    "chat(\"bro\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
