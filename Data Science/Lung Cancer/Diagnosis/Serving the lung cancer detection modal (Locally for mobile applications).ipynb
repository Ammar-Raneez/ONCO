{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving the lung cancer detection modal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from flask import Flask, redirect, url_for, request, render_template, jsonify\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import uuid\n",
    "from pyrebase import pyrebase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the flask app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading your saved modal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded. Check http://127.0.0.1:5000/ or http://localhost:5000/\n"
     ]
    }
   ],
   "source": [
    "MODEL_PATH = \"model_vgg16.h5\"\n",
    "model = tf.keras.models.load_model('model_vgg16.h5')\n",
    "print('Model loaded. Check http://127.0.0.1:5000/ or http://localhost:5000/')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict using the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(img_path, model):\n",
    "    IMG_SIZE = 224\n",
    "    img_array = cv2.imread(img_path)\n",
    "    new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE), 3)\n",
    "    new_array = new_array.reshape(1, 224, 224, 3)\n",
    "    prediction = model.predict([new_array])\n",
    "    return prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important GradCAM functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_array(img_path, size):\n",
    "    # `img` is a PIL image of size 299x299\n",
    "    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n",
    "    # `array` is a float32 Numpy array of shape (299, 299, 3)\n",
    "    array = keras.preprocessing.image.img_to_array(img)\n",
    "    # We add a dimension to transform our array into a \"batch\"\n",
    "    # of size (1, 299, 299, 3)\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    return array\n",
    "\n",
    "def make_gradcam_heatmap(\n",
    "    img_array, model, last_conv_layer_name, classifier_layer_names\n",
    "):\n",
    "    # First, we create a model that maps the input image to the activations\n",
    "    # of the last conv layer\n",
    "    last_conv_layer = model.get_layer(last_conv_layer_name)\n",
    "    last_conv_layer_model = keras.Model(model.inputs, last_conv_layer.output)\n",
    "\n",
    "    # Second, we create a model that maps the activations of the last conv\n",
    "    # layer to the final class predictions\n",
    "    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n",
    "    x = classifier_input\n",
    "    for layer_name in classifier_layer_names:\n",
    "        x = model.get_layer(layer_name)(x)\n",
    "    classifier_model = keras.Model(classifier_input, x)\n",
    "\n",
    "    # Then, we compute the gradient of the top predicted class for our input image\n",
    "    # with respect to the activations of the last conv layer\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Compute activations of the last conv layer and make the tape watch it\n",
    "        last_conv_layer_output = last_conv_layer_model(img_array)\n",
    "        tape.watch(last_conv_layer_output)\n",
    "        # Compute class predictions\n",
    "        preds = classifier_model(last_conv_layer_output)\n",
    "        top_pred_index = tf.argmax(preds[0])\n",
    "        top_class_channel = preds[:, top_pred_index]\n",
    "\n",
    "    # This is the gradient of the top predicted class with regard to\n",
    "    # the output feature map of the last conv layer\n",
    "    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n",
    "\n",
    "    # This is a vector where each entry is the mean intensity of the gradient\n",
    "    # over a specific feature map channel\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # We multiply each channel in the feature map array\n",
    "    # by \"how important this channel is\" with regard to the top predicted class\n",
    "    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n",
    "    pooled_grads = pooled_grads.numpy()\n",
    "    for i in range(pooled_grads.shape[-1]):\n",
    "        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n",
    "\n",
    "    # The channel-wise mean of the resulting feature map\n",
    "    # is our heatmap of class activation\n",
    "    heatmap = np.mean(last_conv_layer_output, axis=-1)\n",
    "\n",
    "    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n",
    "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the Visualized GradCAM image to firebase storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def storeGradCamImage(local_target_image_path):\n",
    "   \n",
    "    img_size = (224, 224)\n",
    "    preprocess_input = keras.applications.xception.preprocess_input\n",
    "    decode_predictions = keras.applications.xception.decode_predictions\n",
    "\n",
    "    last_conv_layer_name = \"block5_pool\"\n",
    "    classifier_layer_names = [\n",
    "        \"flatten\",\n",
    "        \"dense\",\n",
    "    ]\n",
    "    \n",
    "    # The local path to our target image\n",
    "    img_path = local_target_image_path\n",
    "    \n",
    "    # Prepare image\n",
    "    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n",
    "    \n",
    "    # Generate class activation heatmap\n",
    "    heatmap = make_gradcam_heatmap(\n",
    "        img_array, model, last_conv_layer_name, classifier_layer_names\n",
    "    )\n",
    "    \n",
    "    # We load the original image\n",
    "    img = keras.preprocessing.image.load_img(img_path)\n",
    "    img = keras.preprocessing.image.img_to_array(img)\n",
    "    \n",
    "    # We rescale heatmap to a range 0-255\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "\n",
    "    # We use jet colormap to colorize heatmap\n",
    "    jet = cm.get_cmap(\"jet\")\n",
    "\n",
    "    # We use RGB values of the colormap\n",
    "    jet_colors = jet(np.arange(256))[:, :3]\n",
    "    jet_heatmap = jet_colors[heatmap]\n",
    "    \n",
    "    # We create an image with RGB colorized heatmap\n",
    "    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n",
    "    jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))\n",
    "    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n",
    "    \n",
    "    # Superimpose the heatmap on original image\n",
    "    superimposed_img = jet_heatmap * 0.4 + img\n",
    "    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n",
    "    \n",
    "    # Save the superimposed image to firebase cloud storage\n",
    "    extension = \".jpg\"\n",
    "    generateImageName = str(uuid.uuid4())\n",
    "    fileName = generateImageName + extension\n",
    "    folderName = \"superimposedImages\"\n",
    "    save_local_path = \"superimposedImages/\" + generateImageName + extension\n",
    "    superimposed_img.save(save_local_path)\n",
    "    \n",
    "   # firebase cloud storage stuff to store the image to cloud\n",
    "    firebase_config = {\n",
    "        \"apiKey\": \"AIzaSyCTCf6EWg_Mthy590c8JpkX5CsOdIlpEA4\",\n",
    "        \"authDomain\": \"onco-127df.firebaseapp.com\",\n",
    "        \"projectId\": \"onco-127df\",\n",
    "        \"databaseURL\": \"https://onco-127df-default-rtdb.firebaseio.com\",\n",
    "        \"storageBucket\": \"onco-127df.appspot.com\",\n",
    "        \"messagingSenderId\": \"425803825049\",\n",
    "        \"appId\": \"1:425803825049:web:3f261cd2ecf11241065d08\",\n",
    "        \"measurementId\": \"G-S3DZNYJLXT\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    firebase = pyrebase.initialize_app(firebase_config)\n",
    "    firebase_storage = firebase.storage()\n",
    "\n",
    "    # storing the image from local path to the firebase cloud storage\n",
    "    firebase_storage.child(folderName + \"/\" + fileName).put(save_local_path)\n",
    "    \n",
    "    # returning the file name of the image I have stored inside the firebase cloud storage\n",
    "    return generateImageName + extension\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the index route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/', methods=['GET'])\n",
    "def index():\n",
    "    return jsonify(\n",
    "        message = \"Hello World\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Prediction Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePredictionPercent(image_path):\n",
    "    prediction = model_predict(image_path, model)\n",
    "    return str(np.amax(prediction[0][1] * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the predict route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route('/predict', methods=['GET', 'POST'])\n",
    "def upload():\n",
    "    if request.method == 'POST':\n",
    "        # Get the file from post request\n",
    "        f = request.files['file']\n",
    "\n",
    "        # Save the file to ./uploads\n",
    "        file_path = \"./uploads/\"+f.filename\n",
    "        print(file_path)\n",
    "        f.save(file_path)\n",
    "\n",
    "        # Storing the image into firebase\n",
    "        image_fileName = storeGradCamImage(file_path);\n",
    "        \n",
    "        # Getting the prediction percentage value\n",
    "        prediction_percentage = calculatePredictionPercent(file_path)\n",
    "        \n",
    "        # Make prediction\n",
    "        prediction = model_predict(file_path, model)\n",
    "\n",
    "        # These are the prediction categories \n",
    "        CATEGORIES = ['CANCER', 'NORMAL']\n",
    "        \n",
    "        # getting the prediction result from the categories\n",
    "        output = CATEGORIES[int(round(prediction[0][0]))]\n",
    "        \n",
    "        # returning the result\n",
    "        return jsonify(\n",
    "            result = output,\n",
    "            imageFileName = image_fileName,\n",
    "            percentage = prediction_percentage\n",
    "        )\n",
    "    \n",
    "    # if not a 'POST' request we then return None\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the main application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", port=80)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 21-22: truncated \\uXXXX escape (<ipython-input-25-0b5d053bad99>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-25-0b5d053bad99>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    file = \"Lung Cancer\\Diagnosis\\uploads\\image_picker2329454639435085364.jpg\"\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 21-22: truncated \\uXXXX escape\n"
     ]
    }
   ],
   "source": [
    "file = \"Lung Cancer\\Diagnosis\\uploads\\image_picker2329454639435085364.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
